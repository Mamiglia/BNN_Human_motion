{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AX0NYqgpU4US"
      },
      "outputs": [],
      "source": [
        "!rm -r ./*\n",
        "!git clone https://github.com/Mamiglia/BNN_Human_motion\n",
        "!mv BNN_Human_motion/* ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vg0gq1CA2nkd"
      },
      "outputs": [],
      "source": [
        "!pip install bayesian-torch lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ww9sB4Io2c8J"
      },
      "outputs": [],
      "source": [
        "from funcs.dataloader import load_dataset\n",
        "from funcs.pos_embed_p import Pos_Embed\n",
        "from funcs.loss import *\n",
        "from funcs.utils import h36motion3d as datasets\n",
        "from funcs.utils.data_utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oc9MNwufU94V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY0qGeWG8eNQ",
        "outputId": "7a928b1a-753b-467a-8995-328d3f36f2d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda - Type: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device,  '- Type:', torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT4EwVgTkhhw",
        "outputId": "2c461c7e-c1c6-4859-d37d-12715920666b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Train Dataset...\n",
            "Loading Validation Dataset...\n",
            ">>> Training dataset length: 181577\n",
            ">>> Validation dataset length: 28410\n"
          ]
        }
      ],
      "source": [
        "dataset, vald_dataset = load_dataset(output_n=15, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2xJCGqoloFt"
      },
      "source": [
        "# Costum Model from HW3\n",
        "- Encoder: Transfomer + CNN\n",
        "- Decoder: LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RIJSktVmf_U"
      },
      "source": [
        "## Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHIPyFkRl24A"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fkV-z-A9mCag"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    # CLASSICAL ATTENTION MECHANISM\n",
        "    # IT'S THE SAME OF THE TEORICAL PART\n",
        "\n",
        "    def __init__(self, attn_dropout):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        attn = torch.matmul(query, key.transpose(-2, -1))\n",
        "        d_k = query.size(-1)\n",
        "        attn = attn / (d_k ** 0.5)\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e9)\n",
        "        attn = self.dropout(F.softmax(attn,-1))\n",
        "        output = torch.matmul(attn, value)\n",
        "        return output, attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UqbYj72GmCPU"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    # ALSO THE MULTIHEAD ATTENTION MECHANISM IS THE SAME OF TEORICAL PART\n",
        "\n",
        "    def __init__(self, num_heads, d_model, dropout):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        #  We assume d_v always equals d_k\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.query_ff = nn.Linear(d_model, d_model)\n",
        "        self.key_ff = nn.Linear(d_model, d_model)\n",
        "        self.value_ff = nn.Linear(d_model, d_model)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.attention = Attention(attn_dropout=dropout)\n",
        "    def forward(self, query, key, value, mask=None, return_attention=False):\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        query = self.query_ff(query).view(nbatches, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        key = self.key_ff(key).view(nbatches, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        value = self.value_ff(value).view(nbatches, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        x, self.attn = self.attention(query, key, value, mask)\n",
        "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.num_heads * self.d_k)\n",
        "        if return_attention:\n",
        "            return self.attn_ff(x), self.attn\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NKAjIztjmBBL"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    # THE ENCODER BLOCK IS A TIPICAL TRANSFORMER ENCODER,\n",
        "    # BUT WE HAVE ADDED A CONVOLUTIONAL LAYER AT THE END:\n",
        "    # self.conv IS USED TO SHRINK THE INFORMATION ALONG THE ENCODER BLOCKS\n",
        "    # THIS IS A METHOD TO DISTILL INFORMATION SHOULD BE PASSED TO THE DECODER\n",
        "    # AND MOREOVER TO SAVE A LOT OF TIME\n",
        "\n",
        "    def __init__(self,num_heads,d_model,time_in,time_out,num_joints,dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_joints = num_joints\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(num_heads, d_model, dropout)\n",
        "\n",
        "        # LAYERNORM LAYERS AND DROPOUT\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # MLP\n",
        "        self.lin_net = nn.Sequential(\n",
        "            nn.Linear(d_model,2*d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2*d_model,d_model),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # A CNN SHRINKS THE FRAMES FROM time_it TO time_out\n",
        "        # AND THE d_model IS HALVED AFTER EACH BLOCK\n",
        "        self.conv = nn.Conv1d(time_in,time_out,3,padding=1)\n",
        "\n",
        "    def forward(self, xs, mask=None):\n",
        "        x = xs\n",
        "        att = self.self_attn(x, x, x, mask)\n",
        "        x = self.relu(x + att)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        lin_output = self.lin_net(x)\n",
        "        x = self.relu(x + self.dropout(lin_output))\n",
        "\n",
        "        # WE ADD A RESIDUAL CONNECTION ALSO HERE\n",
        "        # IT REDUCES A BIT THE VANISH GRADIENT\n",
        "        x = self.norm2(x + xs)\n",
        "\n",
        "        # SHRINK\n",
        "        x = self.conv(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upCttpEdmhQ2"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPogCINwmIgb"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Ga-9vyI2lz7E"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    # THE DECODER BLOCK IS A SIMPLE LSTM,\n",
        "    # WHICH TAKES THE DISTILLED INFORMATION FROM THE\n",
        "    # ENCODER AS HIDDEN STATE AND AUTOREGRESSIVLY\n",
        "    # FORSEES THE NEXT FRAMES\n",
        "    def __init__(self, hidden_dim, num_layers=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        # LSTM\n",
        "        self.lstm = nn.LSTM(66, hidden_dim, num_layers=num_layers)\n",
        "        # LINEAR self.out TO RETURN THE ORIGINAL DIMENSIONS\n",
        "        self.out = nn.Linear(hidden_dim, 66)\n",
        "\n",
        "    def forward(self, hidden, num_steps):\n",
        "        # CELL STATE IS INITIALIZED TO ZEROS\n",
        "        cell = nn.Parameter(torch.zeros(*hidden.shape)).to(device)\n",
        "        batch_size = hidden.size(1)\n",
        "        # THE <START> IS INITIALIZED TO ONES\n",
        "        input = torch.ones((batch_size,66), dtype=torch.float).unsqueeze(0).to(device)\n",
        "        outputs = torch.zeros((num_steps,batch_size,66), dtype=torch.float).to(device)\n",
        "\n",
        "        for t in range(num_steps):\n",
        "            # FORCAST\n",
        "            decoder_output, (_,_) = self.lstm(input, (hidden,cell))\n",
        "            # SET TO THE ORIGINAL DIMENSIONS\n",
        "            decoder_output = self.out(decoder_output[-1])\n",
        "            outputs[t] = decoder_output\n",
        "            # STACK TO THE INPUT\n",
        "            input = torch.cat((input,decoder_output.unsqueeze(0)),0)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY2CT5WZmNKI"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0jwpPR8rmUwK"
      },
      "outputs": [],
      "source": [
        "def conv_init(conv):\n",
        "    nn.init.kaiming_normal_(conv.weight, nonlinearity='relu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "SecYT-rjmUoF"
      },
      "outputs": [],
      "source": [
        "def fc_init(fc):\n",
        "    nn.init.kaiming_normal_(fc.weight, nonlinearity='relu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "FT97LMNqmUbD"
      },
      "outputs": [],
      "source": [
        "def bn_init(bn, scale):\n",
        "    nn.init.constant_(bn.weight, scale)\n",
        "    nn.init.constant_(bn.bias, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "F6F99TtgmPN7"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    # THIS IS THE FINAL MODEL IN WHICH WE MERGED THE ENCODER AND DECODER PARTS\n",
        "    def __init__(self, num_channels, num_frames_out,\n",
        "                 old_frames, num_joints, num_heads, drop,\n",
        "                 d_model = 512, config=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # CONFIGURATION FOR THE ENCODER BLOCKS\n",
        "        if config==None:\n",
        "            self.config = [[d_model,10,8],[d_model,8,6],[d_model,6,4],[d_model,4,2],[d_model,2,1]]\n",
        "\n",
        "        self.num_channels = num_channels\n",
        "        self.num_frames_out = num_frames_out\n",
        "        self.num_heads = num_heads\n",
        "        self.num_joints = num_joints\n",
        "        self.old_frames = old_frames\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # LINEAR BLOCK TO PASS FROM INITIAL DIMENSION 66 TO d_model\n",
        "        self.lin = nn.Sequential(nn.Linear(self.num_channels*self.num_joints,d_model),nn.BatchNorm1d(self.old_frames))\n",
        "        self.norm = nn.BatchNorm2d(self.num_channels)\n",
        "\n",
        "        # ENCODER\n",
        "        self.blocks = nn.ModuleList()\n",
        "        # d_ : d_model of the block;\n",
        "        # in_ : number of frames of the sequences at the begin;\n",
        "        # out_ : number of frames of the sequences at the end\n",
        "        for index, (d_,in_,out_) in enumerate(self.config):\n",
        "            self.blocks.append(EncoderBlock(num_heads=self.num_heads,\n",
        "                                            d_model=d_, time_in=in_, time_out=out_,\n",
        "                                            num_joints=self.num_joints,dropout=drop))\n",
        "\n",
        "        # SINUSOIDAL POSITIONAL EMBEDDINGS\n",
        "        self.pos = Pos_Embed(self.num_channels,self.old_frames,self.num_joints)\n",
        "        # DECODER: HIDDEN STATE DIMENSION IS THE DIMENSION OF THE LAST ENCODER\n",
        "        self.dec = Decoder(self.d_model)\n",
        "\n",
        "        # WEIGHTS INITIALIZATION\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                conv_init(m)\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                conv_init(m)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                bn_init(m,1)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                bn_init(m,1)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                fc_init(m)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = x.view(-1,self.old_frames,self.num_joints,self.num_channels).permute(0,3,1,2)\n",
        "        # SUM POSITIONAL EMBEDDING\n",
        "        x = (x + self.pos(x)).permute(0,2,3,1).view(-1,self.old_frames,self.num_joints*self.num_channels)\n",
        "        # LINEAR LAYER\n",
        "        x = self.lin(x)\n",
        "        # ENCODER BLOCKS\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "        # RETRIEVE CONTEXT FROM THE ENCODER\n",
        "        context = x.view(-1,self.d_model).unsqueeze(0)\n",
        "        # PASS TO THE DECODER AND RETURN THE RESULTS\n",
        "        results = self.dec(hidden = context,num_steps = self.num_frames_out)\n",
        "        results = results.permute(1,0,2)\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUL3vfCfmaOO"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "O5MQaXD6mlXI"
      },
      "outputs": [],
      "source": [
        "# Argument for training\n",
        "n_epochs = 5\n",
        "log_step = 100\n",
        "input_n = 10\n",
        "output_n = 15\n",
        "\n",
        "# The model name to save/load\n",
        "datas = 'h36m'\n",
        "model_path = datas + '_3d_' + str(output_n) + 'frames_ckpt'\n",
        "\n",
        "model = Model(num_channels=3,\n",
        "              num_frames_out=output_n,\n",
        "              old_frames=input_n,\n",
        "              num_joints=22,\n",
        "              num_heads=8,\n",
        "              drop=0.3).to(device)\n",
        "\n",
        "# Arguments to setup the optimizer\n",
        "lr = 5e-04 # learning rate\n",
        "use_scheduler = True # use MultiStepLR scheduler\n",
        "milestones = [2, 2, 2, 5, 5]   # the epochs after which the learning rate is adjusted by gamma\n",
        "gamma = 0.5 #gamma correction to the learning rate, after reaching the milestone epochs\n",
        "weight_decay = 0.00003 # weight decay (L2 penalty)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "if use_scheduler:\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "v0IV0mwYmnkj"
      },
      "outputs": [],
      "source": [
        "# WE HAVE REWRITE THE TRAIN FUNCTION\n",
        "# TO IMPLEMENT THE SPEED REPRESENTATION\n",
        "\n",
        "save_and_plot = True\n",
        "def train_final(data_loader, vald_loader, clip_grad=None):\n",
        "  train_loss = []\n",
        "  val_loss = []\n",
        "  val_loss_best = 1000\n",
        "\n",
        "  # Initialize lists to store data from each checkpoint\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  dim_used = np.array([6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 22, 23, 24, 25,\n",
        "                    26, 27, 28, 29, 30, 31, 32, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45,\n",
        "                    46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 63, 64, 65, 66, 67, 68,\n",
        "                    75, 76, 77, 78, 79, 80, 81, 82, 83, 87, 88, 89, 90, 91, 92])\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "      running_loss = 0\n",
        "      n = 0\n",
        "      model.train()\n",
        "      for cnt, batch in enumerate(data_loader):\n",
        "          batch = batch.float().to(device)\n",
        "          batch_dim = batch.shape[0]\n",
        "          n += batch_dim\n",
        "\n",
        "          # GET SPEED REPRESENTATION:\n",
        "          # SET THE FIRST FRAME TO ZERO AND CALCULATE THE SPEED AS\n",
        "          # Y_{N+1} = X_{N+1} - X_{N}\n",
        "          sequences_train = torch.cat((torch.zeros(*batch[:, :1, dim_used].size()).to(device),batch[:, 1:10, dim_used]-batch[:, :9, dim_used]), 1)\n",
        "          sequences_gt = batch[:, input_n:input_n + output_n, dim_used]\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          sequences_predict=model(sequences_train)\n",
        "          # COME BACK POSITIONAL REPRESENTATION:\n",
        "          # SUM EACH FRAME TO THE NEXT\n",
        "          # X_{N+1} = Y_{N+1} + Y_{N}\n",
        "          # ADD THE LAST FRAME OF THE TRAIN ONES\n",
        "          sequences_predict[:, 1:output_n, :] = sequences_predict[:, 1:output_n, :] + sequences_predict[:, :output_n-1, :]\n",
        "          sequences_predict = (sequences_predict + batch[:, (input_n-1):input_n, dim_used])\n",
        "\n",
        "\n",
        "          loss = mpjpe_error(sequences_predict, sequences_gt) / output_n\n",
        "\n",
        "\n",
        "          if cnt % log_step == 0:\n",
        "            print('[Epoch: %d, Iteration: %5d]  Training loss: %.3f' %(epoch+1, cnt+1, loss.item()*output_n))\n",
        "\n",
        "          loss.backward()\n",
        "          if clip_grad is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "\n",
        "          optimizer.step()\n",
        "          running_loss += loss*batch_dim*output_n\n",
        "\n",
        "      train_loss.append(running_loss.detach().cpu()/n)\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          running_loss = 0\n",
        "          n = 0\n",
        "          for cnt, batch in enumerate(vald_loader):\n",
        "              batch = batch.float().to(device)\n",
        "              batch_dim = batch.shape[0]\n",
        "              n += batch_dim\n",
        "\n",
        "              # GET SPEED REPRESENTATION:\n",
        "              # SET THE FIRST FRAME TO ZERO AND CALCULATE THE SPEED AS\n",
        "              # Y_{N+1} = X_{N+1} - X_{N}\n",
        "              sequences_train = torch.cat((torch.zeros(*batch[:, :1, dim_used].size()).to(device), batch[:, 1:input_n, dim_used] - batch[:, :input_n-1, dim_used]), 1)\n",
        "              sequences_gt = batch[:, input_n:input_n + output_n, dim_used]\n",
        "\n",
        "              sequences_predict = model(sequences_train)\n",
        "\n",
        "              # COME BACK POSITIONAL REPRESENTATION:\n",
        "              # SUM EACH FRAME TO THE NEXT\n",
        "              # X_{N+1} = Y_{N+1} + Y_{N}\n",
        "              # ADD THE LAST FRAME OF THE TRAIN ONES\n",
        "              sequences_predict[:, 1:output_n, :] = sequences_predict[:, 1:output_n, :] + sequences_predict[:, :(output_n-1), :]\n",
        "              sequences_predict = (sequences_predict + batch[:, (input_n-1):input_n, dim_used])\n",
        "              loss = mpjpe_error(sequences_predict, sequences_gt) / output_n\n",
        "\n",
        "              if cnt % log_step == 0:\n",
        "                print('[Epoch: %d, Iteration: %5d]  Validation loss: %.3f' %(epoch+1, cnt+1, loss.item()*output_n))\n",
        "              running_loss += loss * batch_dim * output_n\n",
        "          val_loss.append(running_loss.detach().cpu()/n)\n",
        "          if running_loss/n < val_loss_best:\n",
        "            val_loss_best = running_loss/n\n",
        "          if (epoch+1) % 5 == 0:\n",
        "            torch.save(model.state_dict(), './checkpoints/LSTM_final_checkpoint_' + str(epoch+1) + '.pt')\n",
        "            train_losses.append(train_loss[-1])\n",
        "            val_losses.append(val_loss[-1])\n",
        "\n",
        "  if use_scheduler:\n",
        "    scheduler.step()\n",
        "\n",
        "  epochs=[5]#,10,15,20]\n",
        "\n",
        "  # Create the plot\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(epochs, train_losses, label='Train Loss', marker='o')\n",
        "  plt.plot(epochs, val_losses, label='Validation Loss', marker='o')\n",
        "\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Training and Validation Loss Over Epochs')\n",
        "\n",
        "  plt.legend()\n",
        "\n",
        "  # Display the plot\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "  return train_losses,val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "WxadEGHsmp7n",
        "outputId": "220f9e4e-3ce9-4e1b-e2b5-ad1a959580ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch: 1, Iteration:     1]  Training loss: 89.595\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-f03c63ebe02f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvald_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-a69bb6282966>\u001b[0m in \u001b[0;36mtrain_final\u001b[0;34m(data_loader, vald_loader, clip_grad)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[Epoch: %d, Iteration: %5d]  Training loss: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moutput_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mclip_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "t_loss, v_loss = train_final(dataset, vald_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grqfpjoQmsgf"
      },
      "source": [
        "### Analysis and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_ZNNWfZmw9_"
      },
      "outputs": [],
      "source": [
        "def test(ckpt_path=None):\n",
        "    model.load_state_dict(torch.load(ckpt_path))\n",
        "    print('model loaded')\n",
        "    model.eval()\n",
        "    accum_loss = 0\n",
        "    n_batches = 0\n",
        "    actions = define_actions(actions_to_consider_test)\n",
        "    dim_used = np.array([ 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 22, 23, 24, 25,\n",
        "                          26, 27, 28, 29, 30, 31, 32, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45,\n",
        "                          46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 63, 64, 65, 66, 67, 68,\n",
        "                          75, 76, 77, 78, 79, 80, 81, 82, 83, 87, 88, 89, 90, 91, 92 ])\n",
        "    # joints at same loc\n",
        "    joint_to_ignore = np.array([16, 20, 23, 24, 28, 31])\n",
        "\n",
        "    index_to_ignore = np.concatenate((joint_to_ignore * 3, joint_to_ignore * 3 + 1, joint_to_ignore * 3 + 2))\n",
        "    joint_equal = np.array([13, 19, 22, 13, 27, 30])\n",
        "    index_to_equal = np.concatenate((joint_equal*3, joint_equal*3+1, joint_equal*3+2))\n",
        "    totalll = 0\n",
        "    counter = 0\n",
        "    for action in actions:\n",
        "      running_loss = 0\n",
        "      n = 0\n",
        "      dataset_test = datasets.Datasets(path, input_n, 25, skip_rate, split=2, actions=[action])\n",
        "      #print('>>> test action for sequences: {:d}'.format(dataset_test.__len__()))\n",
        "\n",
        "      test_loader = DataLoader(dataset_test, batch_size=batch_size_test, shuffle=False, num_workers=0, pin_memory=True)\n",
        "      for cnt,batch in enumerate(test_loader):\n",
        "        with torch.no_grad():\n",
        "\n",
        "          batch=batch.to(device)\n",
        "          batch_dim=batch.shape[0]\n",
        "          n+=batch_dim\n",
        "\n",
        "          all_joints_seq=batch.clone()[:, input_n:input_n+25,:]\n",
        "\n",
        "          # GET SPEED REPRESENTATION OF THE FIRST FRAMES:\n",
        "          # SET THE FIRST FRAME TO ZERO AND CALCULATE THE SPEED AS\n",
        "          # Y_{N+1} = X_{N+1} - X_{N}\n",
        "          sequences_train1=torch.cat((torch.zeros(*batch[:,:1,dim_used].size()).to(device),batch[:,1:input_n,dim_used]-batch[:,:(input_n-1),dim_used]),1)\n",
        "          sequences_gt=batch[:,10:35,:]\n",
        "\n",
        "\n",
        "          running_time = time.time()\n",
        "          sequences_predict1 = model(sequences_train1)\n",
        "\n",
        "          # COME BACK POSITIONAL REPRESENTATION OF THE FIRST PREDICTION:\n",
        "          # SUM EACH FRAME TO THE NEXT\n",
        "          # X_{N+1} = Y_{N+1} + Y_{N}\n",
        "          # ADD THE LAST FRAME OF THE TRAIN ONES\n",
        "          sequences_predict1[:,1:output_n,:]=sequences_predict1[:,1:output_n,:]+sequences_predict1[:,:(output_n-1),:]\n",
        "          sequences_predict1=(sequences_predict1+batch[:,(input_n-1):input_n,dim_used])\n",
        "\n",
        "          # GET SPEED REPRESENTATION OF THE PREDICTED STEPS:\n",
        "          # SET THE FIRST FRAME TO ZERO AND CALCULATE THE SPEED AS\n",
        "          # Y_{N+1} = X_{N+1} - X_{N}\n",
        "          sequences_train2=torch.cat((torch.zeros(*batch[:,:1,dim_used].size()).to(device),\n",
        "                                     sequences_predict1[:,6:15,:]-sequences_predict1[:,5:14,:]),1)\n",
        "\n",
        "          sequences_predict2=model(sequences_train2)\n",
        "\n",
        "          # COME BACK POSITIONAL REPRESENTATION OF THE SECOND PREDICTION:\n",
        "          # SUM EACH FRAME TO THE NEXT\n",
        "          # X_{N+1} = Y_{N+1} + Y_{N}\n",
        "          # ADD THE LAST FRAME OF THE TRAIN ONES\n",
        "          sequences_predict2[:,1:output_n,:]=sequences_predict2[:,1:output_n,:]+sequences_predict2[:,:(output_n-1),:]\n",
        "          sequences_predict2=(sequences_predict2+sequences_predict1[:,(output_n-1):output_n,:])\n",
        "\n",
        "          # STACK THE TWO PREDICTED SEQUENCES\n",
        "          sequences_predict = torch.cat((sequences_predict1,sequences_predict2[:,:10,:]),1)\n",
        "\n",
        "          totalll += time.time()-running_time\n",
        "          counter += 1\n",
        "\n",
        "          all_joints_seq[:,:,dim_used] = sequences_predict\n",
        "\n",
        "\n",
        "          all_joints_seq[:,:,index_to_ignore] = all_joints_seq[:,:,index_to_equal]\n",
        "\n",
        "          loss = mpjpe_error(all_joints_seq.view(-1,25,32,3),sequences_gt.view(-1,25,32,3))\n",
        "          running_loss += loss*batch_dim\n",
        "          accum_loss += loss*batch_dim\n",
        "\n",
        "      #print('loss at test subject for action : '+str(action)+ ' is: '+ str(running_loss/n))\n",
        "      print(str(action),': ', str(np.round((running_loss/n).item(),1)))\n",
        "      n_batches+=n\n",
        "    print('Average: ' + str(np.round((accum_loss/n_batches).item(),1)))\n",
        "    print('Prediction time: ', totalll/counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yz9TMbI3mxfh"
      },
      "outputs": [],
      "source": [
        "path = './data/h3.6m/h3.6m/dataset'\n",
        "skip_rate = 1\n",
        "batch_size_test = 8\n",
        "actions_to_consider_test = 'all'\n",
        "ckpt_path = './checkpoints/LSTM_final_checkpoint_5.pt'\n",
        "\n",
        "test(ckpt_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67awsFbxm9dn"
      },
      "source": [
        "# Bayesian LSTM version\n",
        "- Encoder: Transformer + CNN\n",
        "- Decoder: Bayesian LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN5xLxk0otQN"
      },
      "outputs": [],
      "source": [
        "!pip install blitz-bayesian-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZwIkuRgooubO"
      },
      "outputs": [],
      "source": [
        "# to use the blitz-bayesian library\n",
        "from blitz.modules import BayesianLinear\n",
        "from blitz.modules import BayesianLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LFWVgbfg4l0x"
      },
      "outputs": [],
      "source": [
        "# to use the bayesian-torch library\n",
        "from bayesian_torch.layers import LinearReparameterization as BayesianLinear\n",
        "from bayesian_torch.layers import LSTMReparameterization as BayesianLSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX8M0Sa_AVQ5"
      },
      "source": [
        "## Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RStWekR7AVRB"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tN8KjEMpAVRB"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    # CLASSICAL ATTENTION MECHANISM\n",
        "    # IT'S THE SAME OF THE TEORICAL PART\n",
        "\n",
        "    def __init__(self, attn_dropout):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        attn = torch.matmul(query, key.transpose(-2, -1))\n",
        "        d_k = query.size(-1)\n",
        "        attn = attn / (d_k ** 0.5)\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e9)\n",
        "        attn = self.dropout(F.softmax(attn,-1))\n",
        "        output = torch.matmul(attn, value)\n",
        "        return output, attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "amaoykvRAVRB"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    # ALSO THE MULTIHEAD ATTENTION MECHANISM IS THE SAME OF TEORICAL PART\n",
        "\n",
        "    def __init__(self, num_heads, d_model, dropout):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        #  We assume d_v always equals d_k\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.query_ff = nn.Linear(d_model, d_model)\n",
        "        self.key_ff = nn.Linear(d_model, d_model)\n",
        "        self.value_ff = nn.Linear(d_model, d_model)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.attention = Attention(attn_dropout=dropout)\n",
        "    def forward(self, query, key, value, mask=None, return_attention=False):\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        query = self.query_ff(query).view(nbatches, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        key = self.key_ff(key).view(nbatches, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        value = self.value_ff(value).view(nbatches, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        x, self.attn = self.attention(query, key, value, mask)\n",
        "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.num_heads * self.d_k)\n",
        "        if return_attention:\n",
        "            return self.attn_ff(x), self.attn\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jA0GYRIYAVRB"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    # THE ENCODER BLOCK IS A TIPICAL TRANSFORMER ENCODER,\n",
        "    # BUT WE HAVE ADDED A CONVOLUTIONAL LAYER AT THE END:\n",
        "    # self.conv IS USED TO SHRINK THE INFORMATION ALONG THE ENCODER BLOCKS\n",
        "    # THIS IS A METHOD TO DISTILL INFORMATION SHOULD BE PASSED TO THE DECODER\n",
        "    # AND MOREOVER TO SAVE A LOT OF TIME\n",
        "\n",
        "    def __init__(self,num_heads,d_model,time_in,time_out,num_joints,dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_joints = num_joints\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(num_heads, d_model, dropout)\n",
        "\n",
        "        # LAYERNORM LAYERS AND DROPOUT\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # MLP\n",
        "        self.lin_net = nn.Sequential(\n",
        "            nn.Linear(d_model,2*d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2*d_model,d_model),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # A CNN SHRINKS THE FRAMES FROM time_it TO time_out\n",
        "        # AND THE d_model IS HALVED AFTER EACH BLOCK\n",
        "        self.conv = nn.Conv1d(time_in,time_out,3,padding=1)\n",
        "\n",
        "    def forward(self, xs, mask=None):\n",
        "        x = xs\n",
        "        att = self.self_attn(x, x, x, mask)\n",
        "        x = self.relu(x + att)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        lin_output = self.lin_net(x)\n",
        "        x = self.relu(x + self.dropout(lin_output))\n",
        "\n",
        "        # WE ADD A RESIDUAL CONNECTION ALSO HERE\n",
        "        # IT REDUCES A BIT THE VANISH GRADIENT\n",
        "        x = self.norm2(x + xs)\n",
        "\n",
        "        # SHRINK\n",
        "        x = self.conv(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7uoY-P7AVRC"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YK_aIjVwAVRC"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    # THE DECODER BLOCK IS A SIMPLE LSTM,\n",
        "    # WHICH TAKES THE DISTILLED INFORMATION FROM THE\n",
        "    # ENCODER AS HIDDEN STATE AND AUTOREGRESSIVLY\n",
        "    # FORSEES THE NEXT FRAMES\n",
        "    def __init__(self, hidden_dim, num_layers=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        # LSTM\n",
        "        self.lstm = BayesianLSTM(66, hidden_dim)\n",
        "        # LINEAR self.out TO RETURN THE ORIGINAL DIMENSIONS\n",
        "        self.out = nn.Linear(hidden_dim, 66)\n",
        "\n",
        "    def forward(self, hidden, num_steps):\n",
        "        # CELL STATE IS INITIALIZED TO ZEROS\n",
        "        cell = nn.Parameter(torch.zeros(*hidden.shape)).to(device)\n",
        "        batch_size = hidden.size(1)\n",
        "        # THE <START> IS INITIALIZED TO ONES\n",
        "        input = torch.ones((batch_size,66), dtype=torch.float).unsqueeze(0).to(device)\n",
        "        outputs = torch.zeros((num_steps,batch_size,66), dtype=torch.float).to(device)\n",
        "\n",
        "        for t in range(num_steps):\n",
        "            # FORCAST\n",
        "            decoder_output, (_,_) = self.lstm(input) # if blitz-bayesian library\n",
        "            #decoder_output, _, kl = self.lstm(input) # if bayesian-torch library\n",
        "            # SET TO THE ORIGINAL DIMENSIONS\n",
        "            decoder_output = self.out(decoder_output[-1])\n",
        "            outputs[t] = decoder_output\n",
        "            # STACK TO THE INPUT\n",
        "            input = torch.cat((input,decoder_output.unsqueeze(0)),0)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nGPOuEoAVRC"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3FPpIS6SAVRC"
      },
      "outputs": [],
      "source": [
        "def conv_init(conv):\n",
        "    nn.init.kaiming_normal_(conv.weight, nonlinearity='relu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VkznwRnmAVRC"
      },
      "outputs": [],
      "source": [
        "def fc_init(fc):\n",
        "    nn.init.kaiming_normal_(fc.weight, nonlinearity='relu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lntSvOeKAVRC"
      },
      "outputs": [],
      "source": [
        "def bn_init(bn, scale):\n",
        "    nn.init.constant_(bn.weight, scale)\n",
        "    nn.init.constant_(bn.bias, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gZdzMIDfAVRC"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    # THIS IS THE FINAL MODEL IN WHICH WE MERGED THE ENCODER AND DECODER PARTS\n",
        "    def __init__(self, num_channels, num_frames_out,\n",
        "                 old_frames, num_joints, num_heads, drop,\n",
        "                 d_model = 512, config=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # CONFIGURATION FOR THE ENCODER BLOCKS\n",
        "        if config==None:\n",
        "            self.config = [[d_model,10,8],[d_model,8,6],[d_model,6,4],[d_model,4,2],[d_model,2,1]]\n",
        "\n",
        "        self.num_channels = num_channels\n",
        "        self.num_frames_out = num_frames_out\n",
        "        self.num_heads = num_heads\n",
        "        self.num_joints = num_joints\n",
        "        self.old_frames = old_frames\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # LINEAR BLOCK TO PASS FROM INITIAL DIMENSION 66 TO d_model\n",
        "        self.lin = nn.Sequential(nn.Linear(self.num_channels*self.num_joints,d_model),nn.BatchNorm1d(self.old_frames))\n",
        "        self.norm = nn.BatchNorm2d(self.num_channels)\n",
        "\n",
        "        # ENCODER\n",
        "        self.blocks = nn.ModuleList()\n",
        "        # d_ : d_model of the block;\n",
        "        # in_ : number of frames of the sequences at the begin;\n",
        "        # out_ : number of frames of the sequences at the end\n",
        "        for index, (d_,in_,out_) in enumerate(self.config):\n",
        "            self.blocks.append(EncoderBlock(num_heads=self.num_heads,\n",
        "                                            d_model=d_, time_in=in_, time_out=out_,\n",
        "                                            num_joints=self.num_joints,dropout=drop))\n",
        "\n",
        "        # SINUSOIDAL POSITIONAL EMBEDDINGS\n",
        "        self.pos = Pos_Embed(self.num_channels,self.old_frames,self.num_joints)\n",
        "        # DECODER: HIDDEN STATE DIMENSION IS THE DIMENSION OF THE LAST ENCODER\n",
        "        self.dec = Decoder(self.d_model)\n",
        "\n",
        "        # WEIGHTS INITIALIZATION\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                conv_init(m)\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                conv_init(m)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                bn_init(m,1)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                bn_init(m,1)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                fc_init(m)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = x.view(-1,self.old_frames,self.num_joints,self.num_channels).permute(0,3,1,2)\n",
        "        # SUM POSITIONAL EMBEDDING\n",
        "        x = (x + self.pos(x)).permute(0,2,3,1).view(-1,self.old_frames,self.num_joints*self.num_channels)\n",
        "        # LINEAR LAYER\n",
        "        x = self.lin(x)\n",
        "        # ENCODER BLOCKS\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "        # RETRIEVE CONTEXT FROM THE ENCODER\n",
        "        context = x.view(-1,self.d_model).unsqueeze(0)\n",
        "        # PASS TO THE DECODER AND RETURN THE RESULTS\n",
        "        results = self.dec(hidden = context,num_steps = self.num_frames_out)\n",
        "        results = results.permute(1,0,2)\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF-6ZJHMAVRD"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "M_Rq3_ibAVRD"
      },
      "outputs": [],
      "source": [
        "# Argument for training\n",
        "n_epochs = 5\n",
        "log_step = 50\n",
        "input_n = 10\n",
        "output_n = 15\n",
        "\n",
        "# The model name to save/load\n",
        "datas = 'h36m'\n",
        "model_path = datas + '_3d_' + str(output_n) + 'frames_ckpt'\n",
        "\n",
        "model = Model(num_channels=3,\n",
        "              num_frames_out=output_n,\n",
        "              old_frames=input_n,\n",
        "              num_joints=22,\n",
        "              num_heads=8,\n",
        "              drop=0.3).to(device)\n",
        "\n",
        "# Arguments to setup the optimizer\n",
        "lr = 5e-04 # learning rate\n",
        "use_scheduler = True # use MultiStepLR scheduler\n",
        "milestones = [2, 2, 2, 5, 5]   # the epochs after which the learning rate is adjusted by gamma\n",
        "gamma = 0.5 #gamma correction to the learning rate, after reaching the milestone epochs\n",
        "weight_decay = 0.00003 # weight decay (L2 penalty)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "if use_scheduler:\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZIzedFK6AVRD"
      },
      "outputs": [],
      "source": [
        "# WE HAVE REWRITE THE TRAIN FUNCTION\n",
        "# TO IMPLEMENT THE SPEED REPRESENTATION\n",
        "\n",
        "save_and_plot = False\n",
        "def train_final(data_loader, vald_loader, clip_grad=None):\n",
        "  train_loss = []\n",
        "  val_loss = []\n",
        "  val_loss_best = 1000\n",
        "\n",
        "  # Initialize lists to store data from each checkpoint\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  dim_used = np.array([6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 22, 23, 24, 25,\n",
        "                    26, 27, 28, 29, 30, 31, 32, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45,\n",
        "                    46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 63, 64, 65, 66, 67, 68,\n",
        "                    75, 76, 77, 78, 79, 80, 81, 82, 83, 87, 88, 89, 90, 91, 92])\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "      running_loss = 0\n",
        "      n = 0\n",
        "      model.train()\n",
        "      for cnt, batch in enumerate(data_loader):\n",
        "          batch = batch.float().to(device)\n",
        "          batch_dim = batch.shape[0]\n",
        "          n += batch_dim\n",
        "\n",
        "          # GET SPEED REPRESENTATION:\n",
        "          # SET THE FIRST FRAME TO ZERO AND CALCULATE THE SPEED AS\n",
        "          # Y_{N+1} = X_{N+1} - X_{N}\n",
        "          sequences_train = torch.cat((torch.zeros(*batch[:, :1, dim_used].size()).to(device),batch[:, 1:10, dim_used]-batch[:, :9, dim_used]), 1)\n",
        "          sequences_gt = batch[:, input_n:input_n + output_n, dim_used]\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          sequences_predict=model(sequences_train)\n",
        "          # COME BACK POSITIONAL REPRESENTATION:\n",
        "          # SUM EACH FRAME TO THE NEXT\n",
        "          # X_{N+1} = Y_{N+1} + Y_{N}\n",
        "          # ADD THE LAST FRAME OF THE TRAIN ONES\n",
        "          sequences_predict[:, 1:output_n, :] = sequences_predict[:, 1:output_n, :] + sequences_predict[:, :output_n-1, :]\n",
        "          sequences_predict = (sequences_predict + batch[:, (input_n-1):input_n, dim_used])\n",
        "\n",
        "\n",
        "          loss = mpjpe_error(sequences_predict, sequences_gt) / output_n\n",
        "\n",
        "\n",
        "          if cnt % log_step == 0:\n",
        "            print('[Epoch: %d, Iteration: %5d]  Training loss: %.3f' %(epoch+1, cnt+1, loss.item()*output_n))\n",
        "\n",
        "          loss.backward()\n",
        "          if clip_grad is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "\n",
        "          optimizer.step()\n",
        "          running_loss += loss*batch_dim*output_n\n",
        "\n",
        "      train_loss.append(running_loss.detach().cpu()/n)\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          running_loss = 0\n",
        "          n = 0\n",
        "          for cnt, batch in enumerate(vald_loader):\n",
        "              batch = batch.float().to(device)\n",
        "              batch_dim = batch.shape[0]\n",
        "              n += batch_dim\n",
        "\n",
        "              # GET SPEED REPRESENTATION:\n",
        "              # SET THE FIRST FRAME TO ZERO AND CALCULATE THE SPEED AS\n",
        "              # Y_{N+1} = X_{N+1} - X_{N}\n",
        "              sequences_train = torch.cat((torch.zeros(*batch[:, :1, dim_used].size()).to(device), batch[:, 1:input_n, dim_used] - batch[:, :input_n-1, dim_used]), 1)\n",
        "              sequences_gt = batch[:, input_n:input_n + output_n, dim_used]\n",
        "\n",
        "              sequences_predict = model(sequences_train)\n",
        "\n",
        "              # COME BACK POSITIONAL REPRESENTATION:\n",
        "              # SUM EACH FRAME TO THE NEXT\n",
        "              # X_{N+1} = Y_{N+1} + Y_{N}\n",
        "              # ADD THE LAST FRAME OF THE TRAIN ONES\n",
        "              sequences_predict[:, 1:output_n, :] = sequences_predict[:, 1:output_n, :] + sequences_predict[:, :(output_n-1), :]\n",
        "              sequences_predict = (sequences_predict + batch[:, (input_n-1):input_n, dim_used])\n",
        "              loss = mpjpe_error(sequences_predict, sequences_gt) / output_n\n",
        "\n",
        "              if cnt % log_step == 0:\n",
        "                print('[Epoch: %d, Iteration: %5d]  Validation loss: %.3f' %(epoch+1, cnt+1, loss.item()*output_n))\n",
        "              running_loss += loss * batch_dim * output_n\n",
        "          val_loss.append(running_loss.detach().cpu()/n)\n",
        "          if running_loss/n < val_loss_best:\n",
        "            val_loss_best = running_loss/n\n",
        "          if (epoch+1) % 5 == 0:\n",
        "            torch.save(model.state_dict(), './checkpoints/LSTM_final_checkpoint_' + str(epoch+1) + '.pt')\n",
        "            train_losses.append(train_loss[-1])\n",
        "            val_losses.append(val_loss[-1])\n",
        "\n",
        "  if use_scheduler:\n",
        "    scheduler.step()\n",
        "\n",
        "  epochs=[5]#,10,15,20]\n",
        "\n",
        "  # Create the plot\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(epochs, train_losses, label='Train Loss', marker='o')\n",
        "  plt.plot(epochs, val_losses, label='Validation Loss', marker='o')\n",
        "\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Training and Validation Loss Over Epochs')\n",
        "\n",
        "  plt.legend()\n",
        "\n",
        "  # Display the plot\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "  return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jU2QmreAVRD"
      },
      "outputs": [],
      "source": [
        "t_loss, v_loss = train_final(dataset, vald_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joRCIHl3AVRE"
      },
      "source": [
        "### Analysis and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-NNhaUTAVRE"
      },
      "outputs": [],
      "source": [
        "def test(ckpt_path=None):\n",
        "    model.load_state_dict(torch.load(ckpt_path))\n",
        "    print('model loaded')\n",
        "    model.eval()\n",
        "    accum_loss = 0\n",
        "    n_batches = 0\n",
        "    actions = define_actions(actions_to_consider_test)\n",
        "    dim_used = np.array([ 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 22, 23, 24, 25,\n",
        "                          26, 27, 28, 29, 30, 31, 32, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45,\n",
        "                          46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 63, 64, 65, 66, 67, 68,\n",
        "                          75, 76, 77, 78, 79, 80, 81, 82, 83, 87, 88, 89, 90, 91, 92 ])\n",
        "    # joints at same loc\n",
        "    joint_to_ignore = np.array([16, 20, 23, 24, 28, 31])\n",
        "\n",
        "    index_to_ignore = np.concatenate((joint_to_ignore * 3, joint_to_ignore * 3 + 1, joint_to_ignore * 3 + 2))\n",
        "    joint_equal = np.array([13, 19, 22, 13, 27, 30])\n",
        "    index_to_equal = np.concatenate((joint_equal*3, joint_equal*3+1, joint_equal*3+2))\n",
        "    totalll = 0\n",
        "    counter = 0\n",
        "    for action in actions:\n",
        "      running_loss = 0\n",
        "      n = 0\n",
        "      dataset_test = datasets.Datasets(path, input_n, 25, skip_rate, split=2, actions=[action])\n",
        "      #print('>>> test action for sequences: {:d}'.format(dataset_test.__len__()))\n",
        "\n",
        "      test_loader = DataLoader(dataset_test, batch_size=batch_size_test, shuffle=False, num_workers=0, pin_memory=True)\n",
        "      for cnt,batch in enumerate(test_loader):\n",
        "        with torch.no_grad():\n",
        "\n",
        "          batch=batch.to(device)\n",
        "          batch_dim=batch.shape[0]\n",
        "          n+=batch_dim\n",
        "\n",
        "          all_joints_seq=batch.clone()[:, input_n:input_n+25,:]\n",
        "\n",
        "          # GET SPEED REPRESENTATION OF THE FIRST FRAMES:\n",
        "          # SET THE FIRST FRAME TO ZERO AND CALCULATE THE SPEED AS\n",
        "          # Y_{N+1} = X_{N+1} - X_{N}\n",
        "          sequences_train1=torch.cat((torch.zeros(*batch[:,:1,dim_used].size()).to(device),batch[:,1:input_n,dim_used]-batch[:,:(input_n-1),dim_used]),1)\n",
        "          sequences_gt=batch[:,10:35,:]\n",
        "\n",
        "\n",
        "          running_time = time.time()\n",
        "          sequences_predict1 = model(sequences_train1)\n",
        "\n",
        "          # COME BACK POSITIONAL REPRESENTATION OF THE FIRST PREDICTION:\n",
        "          # SUM EACH FRAME TO THE NEXT\n",
        "          # X_{N+1} = Y_{N+1} + Y_{N}\n",
        "          # ADD THE LAST FRAME OF THE TRAIN ONES\n",
        "          sequences_predict1[:,1:output_n,:]=sequences_predict1[:,1:output_n,:]+sequences_predict1[:,:(output_n-1),:]\n",
        "          sequences_predict1=(sequences_predict1+batch[:,(input_n-1):input_n,dim_used])\n",
        "\n",
        "          # GET SPEED REPRESENTATION OF THE PREDICTED STEPS:\n",
        "          # SET THE FIRST FRAME TO ZERO AND CALCULATE THE SPEED AS\n",
        "          # Y_{N+1} = X_{N+1} - X_{N}\n",
        "          sequences_train2=torch.cat((torch.zeros(*batch[:,:1,dim_used].size()).to(device),\n",
        "                                     sequences_predict1[:,6:15,:]-sequences_predict1[:,5:14,:]),1)\n",
        "\n",
        "          sequences_predict2=model(sequences_train2)\n",
        "\n",
        "          # COME BACK POSITIONAL REPRESENTATION OF THE SECOND PREDICTION:\n",
        "          # SUM EACH FRAME TO THE NEXT\n",
        "          # X_{N+1} = Y_{N+1} + Y_{N}\n",
        "          # ADD THE LAST FRAME OF THE TRAIN ONES\n",
        "          sequences_predict2[:,1:output_n,:]=sequences_predict2[:,1:output_n,:]+sequences_predict2[:,:(output_n-1),:]\n",
        "          sequences_predict2=(sequences_predict2+sequences_predict1[:,(output_n-1):output_n,:])\n",
        "\n",
        "          # STACK THE TWO PREDICTED SEQUENCES\n",
        "          sequences_predict = torch.cat((sequences_predict1,sequences_predict2[:,:10,:]),1)\n",
        "\n",
        "          totalll += time.time()-running_time\n",
        "          counter += 1\n",
        "\n",
        "          all_joints_seq[:,:,dim_used] = sequences_predict\n",
        "\n",
        "\n",
        "          all_joints_seq[:,:,index_to_ignore] = all_joints_seq[:,:,index_to_equal]\n",
        "\n",
        "          loss = mpjpe_error(all_joints_seq.view(-1,25,32,3),sequences_gt.view(-1,25,32,3))\n",
        "          running_loss += loss*batch_dim\n",
        "          accum_loss += loss*batch_dim\n",
        "\n",
        "      #print('loss at test subject for action : '+str(action)+ ' is: '+ str(running_loss/n))\n",
        "      print(str(action),': ', str(np.round((running_loss/n).item(),1)))\n",
        "      n_batches+=n\n",
        "    print('Average: ' + str(np.round((accum_loss/n_batches).item(),1)))\n",
        "    print('Prediction time: ', totalll/counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh_jti0GAVRE"
      },
      "outputs": [],
      "source": [
        "path = './data/h3.6m/h3.6m/dataset'\n",
        "skip_rate = 1\n",
        "batch_size_test = 8\n",
        "actions_to_consider_test = 'all'\n",
        "ckpt_path = './checkpoints/LSTM_final_checkpoint_5.pt'\n",
        "\n",
        "test(ckpt_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pdoyGt6Tceb"
      },
      "source": [
        "# Bayesian GRU version\n",
        "- Encoder: Transformer + CNN\n",
        "- Decoder: Bayesian GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPci27c4Tcej"
      },
      "outputs": [],
      "source": [
        "!pip install blitz-bayesian-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PRpvqSKOTcej"
      },
      "outputs": [],
      "source": [
        "from blitz.modules import BayesianLinear\n",
        "from blitz.modules import BayesianGRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iO4_PjVTcek"
      },
      "source": [
        "## Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUD8j4miTcek"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1GqcL-vTTcek"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    # CLASSICAL ATTENTION MECHANISM\n",
        "    # IT'S THE SAME OF THE TEORICAL PART\n",
        "\n",
        "    def __init__(self, attn_dropout):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        attn = torch.matmul(query, key.transpose(-2, -1))\n",
        "        d_k = query.size(-1)\n",
        "        attn = attn / (d_k ** 0.5)\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask == 0, -1e9)\n",
        "        attn = self.dropout(F.softmax(attn,-1))\n",
        "        output = torch.matmul(attn, value)\n",
        "        return output, attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UCjvwHetTcek"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    # ALSO THE MULTIHEAD ATTENTION MECHANISM IS THE SAME OF TEORICAL PART\n",
        "\n",
        "    def __init__(self, num_heads, d_model, dropout):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        #  We assume d_v always equals d_k\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.query_ff = nn.Linear(d_model, d_model)\n",
        "        self.key_ff = nn.Linear(d_model, d_model)\n",
        "        self.value_ff = nn.Linear(d_model, d_model)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.attention = Attention(attn_dropout=dropout)\n",
        "    def forward(self, query, key, value, mask=None, return_attention=False):\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        query = self.query_ff(query).view(nbatches, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        key = self.key_ff(key).view(nbatches, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        value = self.value_ff(value).view(nbatches, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        x, self.attn = self.attention(query, key, value, mask)\n",
        "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.num_heads * self.d_k)\n",
        "        if return_attention:\n",
        "            return self.attn_ff(x), self.attn\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-OwNQv45Tcel"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    # THE ENCODER BLOCK IS A TIPICAL TRANSFORMER ENCODER,\n",
        "    # BUT WE HAVE ADDED A CONVOLUTIONAL LAYER AT THE END:\n",
        "    # self.conv IS USED TO SHRINK THE INFORMATION ALONG THE ENCODER BLOCKS\n",
        "    # THIS IS A METHOD TO DISTILL INFORMATION SHOULD BE PASSED TO THE DECODER\n",
        "    # AND MOREOVER TO SAVE A LOT OF TIME\n",
        "\n",
        "    def __init__(self,num_heads,d_model,time_in,time_out,num_joints,dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_joints = num_joints\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(num_heads, d_model, dropout)\n",
        "\n",
        "        # LAYERNORM LAYERS AND DROPOUT\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # MLP\n",
        "        self.lin_net = nn.Sequential(\n",
        "            nn.Linear(d_model,2*d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2*d_model,d_model),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # A CNN SHRINKS THE FRAMES FROM time_it TO time_out\n",
        "        # AND THE d_model IS HALVED AFTER EACH BLOCK\n",
        "        self.conv = nn.Conv1d(time_in,time_out,3,padding=1)\n",
        "\n",
        "    def forward(self, xs, mask=None):\n",
        "        x = xs\n",
        "        att = self.self_attn(x, x, x, mask)\n",
        "        x = self.relu(x + att)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        lin_output = self.lin_net(x)\n",
        "        x = self.relu(x + self.dropout(lin_output))\n",
        "\n",
        "        # WE ADD A RESIDUAL CONNECTION ALSO HERE\n",
        "        # IT REDUCES A BIT THE VANISH GRADIENT\n",
        "        x = self.norm2(x + xs)\n",
        "\n",
        "        # SHRINK\n",
        "        x = self.conv(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiaFdbcWTcel"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dDAkSXvRTcel"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    # THE DECODER BLOCK IS A SIMPLE GRU,\n",
        "    # WHICH TAKES THE DISTILLED INFORMATION FROM THE\n",
        "    # ENCODER AS HIDDEN STATE AND AUTOREGRESSIVLY\n",
        "    # FORSEES THE NEXT FRAMES\n",
        "    def __init__(self, hidden_dim, num_layers=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        # GRU\n",
        "        self.gru = BayesianGRU(66, hidden_dim)\n",
        "        # LINEAR self.out TO RETURN THE ORIGINAL DIMENSIONS\n",
        "        self.out = nn.Linear(hidden_dim, 66)\n",
        "\n",
        "    def forward(self, hidden, num_steps):\n",
        "        # CELL STATE IS INITIALIZED TO ZEROS\n",
        "        cell = nn.Parameter(torch.zeros(*hidden.shape)).to(device)\n",
        "        batch_size = hidden.size(1)\n",
        "        # THE <START> IS INITIALIZED TO ONES\n",
        "        input = torch.ones((batch_size,66), dtype=torch.float).unsqueeze(0).to(device)\n",
        "        outputs = torch.zeros((num_steps,batch_size,66), dtype=torch.float).to(device)\n",
        "\n",
        "        for t in range(num_steps):\n",
        "            # FORCAST\n",
        "            decoder_output, _ = self.gru(input)\n",
        "            # SET TO THE ORIGINAL DIMENSIONS\n",
        "            decoder_output = self.out(decoder_output[-1])\n",
        "            outputs[t] = decoder_output\n",
        "            # STACK TO THE INPUT\n",
        "            input = torch.cat((input,decoder_output.unsqueeze(0)),0)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GKuQclGTcel"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "18pzcMrMTcel"
      },
      "outputs": [],
      "source": [
        "def conv_init(conv):\n",
        "    nn.init.kaiming_normal_(conv.weight, nonlinearity='relu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_gFTVDuhTcel"
      },
      "outputs": [],
      "source": [
        "def fc_init(fc):\n",
        "    nn.init.kaiming_normal_(fc.weight, nonlinearity='relu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TIyDDZB3Tcem"
      },
      "outputs": [],
      "source": [
        "def bn_init(bn, scale):\n",
        "    nn.init.constant_(bn.weight, scale)\n",
        "    nn.init.constant_(bn.bias, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UZf12AgDTcem"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    # THIS IS THE FINAL MODEL IN WHICH WE MERGED THE ENCODER AND DECODER PARTS\n",
        "    def __init__(self, num_channels, num_frames_out,\n",
        "                 old_frames, num_joints, num_heads, drop,\n",
        "                 d_model = 512, config=None):\n",
        "        super().__init__()\n",
        "\n",
        "        # CONFIGURATION FOR THE ENCODER BLOCKS\n",
        "        if config==None:\n",
        "            self.config = [[d_model,10,8],[d_model,8,6],[d_model,6,4],[d_model,4,2],[d_model,2,1]]\n",
        "\n",
        "        self.num_channels = num_channels\n",
        "        self.num_frames_out = num_frames_out\n",
        "        self.num_heads = num_heads\n",
        "        self.num_joints = num_joints\n",
        "        self.old_frames = old_frames\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # LINEAR BLOCK TO PASS FROM INITIAL DIMENSION 66 TO d_model\n",
        "        self.lin = nn.Sequential(nn.Linear(self.num_channels*self.num_joints,d_model),nn.BatchNorm1d(self.old_frames))\n",
        "        self.norm = nn.BatchNorm2d(self.num_channels)\n",
        "\n",
        "        # ENCODER\n",
        "        self.blocks = nn.ModuleList()\n",
        "        # d_ : d_model of the block;\n",
        "        # in_ : number of frames of the sequences at the begin;\n",
        "        # out_ : number of frames of the sequences at the end\n",
        "        for index, (d_,in_,out_) in enumerate(self.config):\n",
        "            self.blocks.append(EncoderBlock(num_heads=self.num_heads,\n",
        "                                            d_model=d_, time_in=in_, time_out=out_,\n",
        "                                            num_joints=self.num_joints,dropout=drop))\n",
        "\n",
        "        # SINUSOIDAL POSITIONAL EMBEDDINGS\n",
        "        self.pos = Pos_Embed(self.num_channels,self.old_frames,self.num_joints)\n",
        "        # DECODER: HIDDEN STATE DIMENSION IS THE DIMENSION OF THE LAST ENCODER\n",
        "        self.dec = Decoder(self.d_model)\n",
        "\n",
        "        # WEIGHTS INITIALIZATION\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                conv_init(m)\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                conv_init(m)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                bn_init(m,1)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                bn_init(m,1)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                fc_init(m)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = x.view(-1,self.old_frames,self.num_joints,self.num_channels).permute(0,3,1,2)\n",
        "        # SUM POSITIONAL EMBEDDING\n",
        "        x = (x + self.pos(x)).permute(0,2,3,1).view(-1,self.old_frames,self.num_joints*self.num_channels)\n",
        "        # LINEAR LAYER\n",
        "        x = self.lin(x)\n",
        "        # ENCODER BLOCKS\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "        # RETRIEVE CONTEXT FROM THE ENCODER\n",
        "        context = x.view(-1,self.d_model).unsqueeze(0)\n",
        "        # PASS TO THE DECODER AND RETURN THE RESULTS\n",
        "        results = self.dec(hidden = context,num_steps = self.num_frames_out)\n",
        "        results = results.permute(1,0,2)\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuXx10bGTcem"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "swJ3OAE5Tcem"
      },
      "outputs": [],
      "source": [
        "# Argument for training\n",
        "n_epochs = 5\n",
        "log_step = 50\n",
        "input_n = 10\n",
        "output_n = 15\n",
        "\n",
        "# The model name to save/load\n",
        "datas = 'h36m'\n",
        "model_path = datas + '_3d_' + str(output_n) + 'frames_ckpt'\n",
        "\n",
        "model = Model(num_channels=3,\n",
        "              num_frames_out=output_n,\n",
        "              old_frames=input_n,\n",
        "              num_joints=22,\n",
        "              num_heads=8,\n",
        "              drop=0.3).to(device)\n",
        "\n",
        "# Arguments to setup the optimizer\n",
        "lr = 5e-04 # learning rate\n",
        "use_scheduler = True # use MultiStepLR scheduler\n",
        "milestones = [2, 2, 2, 5, 5]   # the epochs after which the learning rate is adjusted by gamma\n",
        "gamma = 0.5 #gamma correction to the learning rate, after reaching the milestone epochs\n",
        "weight_decay = 0.00003 # weight decay (L2 penalty)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "if use_scheduler:\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ME2u0U_STcem"
      },
      "outputs": [],
      "source": [
        "# WE HAVE REWRITE THE TRAIN FUNCTION\n",
        "# TO IMPLEMENT THE SPEED REPRESENTATION\n",
        "\n",
        "save_and_plot = False\n",
        "def train_final(data_loader, vald_loader, clip_grad=None):\n",
        "  train_loss = []\n",
        "  val_loss = []\n",
        "  val_loss_best = 1000\n",
        "\n",
        "  # Initialize lists to store data from each checkpoint\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  dim_used = np.array([6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 22, 23, 24, 25,\n",
        "                      26, 27, 28, 29, 30, 31, 32, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45,\n",
        "                      46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 63, 64, 65, 66, 67, 68,\n",
        "                      75, 76, 77, 78, 79, 80, 81, 82, 83, 87, 88, 89, 90, 91, 92])\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "      running_loss = 0\n",
        "      n = 0\n",
        "      model.train()\n",
        "      for cnt, batch in enumerate(data_loader):\n",
        "          batch = batch.float().to(device)\n",
        "          batch_dim = batch.shape[0]\n",
        "          n += batch_dim\n",
        "\n",
        "          # GET SPEED REPRESENTATION:\n",
        "          # SET THE FIRST FRAME TO ZERO AND CALCULATE THE SPEED AS\n",
        "          # Y_{N+1} = X_{N+1} - X_{N}\n",
        "          sequences_train = torch.cat((torch.zeros(*batch[:, :1, dim_used].size()).to(device),batch[:, 1:10, dim_used]-batch[:, :9, dim_used]), 1)\n",
        "          sequences_gt = batch[:, input_n:input_n + output_n, dim_used]\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          sequences_predict=model(sequences_train)\n",
        "          # COME BACK POSITIONAL REPRESENTATION:\n",
        "          # SUM EACH FRAME TO THE NEXT\n",
        "          # X_{N+1} = Y_{N+1} + Y_{N}\n",
        "          # ADD THE LAST FRAME OF THE TRAIN ONES\n",
        "          sequences_predict[:, 1:output_n, :] = sequences_predict[:, 1:output_n, :] + sequences_predict[:, :output_n-1, :]\n",
        "          sequences_predict = (sequences_predict + batch[:, (input_n-1):input_n, dim_used])\n",
        "\n",
        "\n",
        "          loss = mpjpe_error(sequences_predict, sequences_gt) / output_n\n",
        "\n",
        "\n",
        "          if cnt % log_step == 0:\n",
        "            print('[Epoch: %d, Iteration: %5d]  Training loss: %.3f' %(epoch+1, cnt+1, loss.item()*output_n))\n",
        "\n",
        "          loss.backward()\n",
        "          if clip_grad is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
        "\n",
        "          optimizer.step()\n",
        "          running_loss += loss*batch_dim*output_n\n",
        "\n",
        "      train_loss.append(running_loss.detach().cpu()/n)\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          running_loss = 0\n",
        "          n = 0\n",
        "          for cnt, batch in enumerate(vald_loader):\n",
        "              batch = batch.float().to(device)\n",
        "              batch_dim = batch.shape[0]\n",
        "              n += batch_dim\n",
        "\n",
        "              # GET SPEED REPRESENTATION:\n",
        "              # SET THE FIRST FRAME TO ZERO AND CALCULATE THE SPEED AS\n",
        "              # Y_{N+1} = X_{N+1} - X_{N}\n",
        "              sequences_train = torch.cat((torch.zeros(*batch[:, :1, dim_used].size()).to(device), batch[:, 1:input_n, dim_used] - batch[:, :input_n-1, dim_used]), 1)\n",
        "              sequences_gt = batch[:, input_n:input_n + output_n, dim_used]\n",
        "\n",
        "              sequences_predict = model(sequences_train)\n",
        "\n",
        "              # COME BACK POSITIONAL REPRESENTATION:\n",
        "              # SUM EACH FRAME TO THE NEXT\n",
        "              # X_{N+1} = Y_{N+1} + Y_{N}\n",
        "              # ADD THE LAST FRAME OF THE TRAIN ONES\n",
        "              sequences_predict[:, 1:output_n, :] = sequences_predict[:, 1:output_n, :] + sequences_predict[:, :(output_n-1), :]\n",
        "              sequences_predict = (sequences_predict + batch[:, (input_n-1):input_n, dim_used])\n",
        "              loss = mpjpe_error(sequences_predict, sequences_gt) / output_n\n",
        "\n",
        "              if cnt % log_step == 0:\n",
        "                print('[Epoch: %d, Iteration: %5d]  Validation loss: %.3f' %(epoch+1, cnt+1, loss.item()*output_n))\n",
        "              running_loss += loss * batch_dim * output_n\n",
        "          val_loss.append(running_loss.detach().cpu()/n)\n",
        "          if running_loss/n < val_loss_best:\n",
        "            val_loss_best = running_loss/n\n",
        "          if (epoch+1) % 5 == 0:\n",
        "            torch.save(model.state_dict(), './checkpoints/LSTM_final_checkpoint_' + str(epoch+1) + '.pt')\n",
        "            train_losses.append(train_loss[-1])\n",
        "            val_losses.append(val_loss[-1])\n",
        "\n",
        "  if use_scheduler:\n",
        "    scheduler.step()\n",
        "\n",
        "  epochs=[5]#,10,15,20]\n",
        "\n",
        "  # Create the plot\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(epochs, train_losses, label='Train Loss', marker='o')\n",
        "  plt.plot(epochs, val_losses, label='Validation Loss', marker='o')\n",
        "\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Training and Validation Loss Over Epochs')\n",
        "\n",
        "  plt.legend()\n",
        "\n",
        "  # Display the plot\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "  return train_losses,val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "Yu80UwR9Tcem",
        "outputId": "1cf2cbf0-be78-4b85-a5bb-3dcf89452aea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch: 1, Iteration:     1]  Training loss: 77.170\n",
            "[Epoch: 1, Iteration:    51]  Training loss: 84.953\n",
            "[Epoch: 1, Iteration:   101]  Training loss: 80.441\n",
            "[Epoch: 1, Iteration:   151]  Training loss: 63.808\n",
            "[Epoch: 1, Iteration:   201]  Training loss: 76.087\n",
            "[Epoch: 1, Iteration:   251]  Training loss: 75.209\n",
            "[Epoch: 1, Iteration:   301]  Training loss: 82.337\n",
            "[Epoch: 1, Iteration:   351]  Training loss: 72.179\n",
            "[Epoch: 1, Iteration:   401]  Training loss: 72.022\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-f03c63ebe02f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvald_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-dc6e9d24febd>\u001b[0m in \u001b[0;36mtrain_final\u001b[0;34m(data_loader, vald_loader, clip_grad)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[Epoch: %d, Iteration: %5d]  Training loss: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moutput_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mclip_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "t_loss, v_loss = train_final(dataset, vald_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uMGVqgGTcen"
      },
      "source": [
        "### Analysis and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BnfwhzjTcen"
      },
      "outputs": [],
      "source": [
        "def test(ckpt_path=None):\n",
        "    model.load_state_dict(torch.load(ckpt_path))\n",
        "    print('model loaded')\n",
        "    model.eval()\n",
        "    accum_loss = 0\n",
        "    n_batches = 0\n",
        "    actions = define_actions(actions_to_consider_test)\n",
        "    dim_used = np.array([ 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 22, 23, 24, 25,\n",
        "                          26, 27, 28, 29, 30, 31, 32, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45,\n",
        "                          46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 63, 64, 65, 66, 67, 68,\n",
        "                          75, 76, 77, 78, 79, 80, 81, 82, 83, 87, 88, 89, 90, 91, 92 ])\n",
        "    # joints at same loc\n",
        "    joint_to_ignore = np.array([16, 20, 23, 24, 28, 31])\n",
        "\n",
        "    index_to_ignore = np.concatenate((joint_to_ignore * 3, joint_to_ignore * 3 + 1, joint_to_ignore * 3 + 2))\n",
        "    joint_equal = np.array([13, 19, 22, 13, 27, 30])\n",
        "    index_to_equal = np.concatenate((joint_equal*3, joint_equal*3+1, joint_equal*3+2))\n",
        "    totalll = 0\n",
        "    counter = 0\n",
        "    for action in actions:\n",
        "      running_loss = 0\n",
        "      n = 0\n",
        "      dataset_test = datasets.Datasets(path, input_n, 25, skip_rate, split=2, actions=[action])\n",
        "      #print('>>> test action for sequences: {:d}'.format(dataset_test.__len__()))\n",
        "\n",
        "      test_loader = DataLoader(dataset_test, batch_size=batch_size_test, shuffle=False, num_workers=0, pin_memory=True)\n",
        "      for cnt,batch in enumerate(test_loader):\n",
        "        with torch.no_grad():\n",
        "\n",
        "          batch=batch.to(device)\n",
        "          batch_dim=batch.shape[0]\n",
        "          n+=batch_dim\n",
        "\n",
        "          all_joints_seq=batch.clone()[:, input_n:input_n+25,:]\n",
        "\n",
        "          # GET SPEED REPRESENTATION OF THE FIRST FRAMES:\n",
        "          # SET THE FIRST FRAME TO ZERO AND CALCULATE THE SPEED AS\n",
        "          # Y_{N+1} = X_{N+1} - X_{N}\n",
        "          sequences_train1=torch.cat((torch.zeros(*batch[:,:1,dim_used].size()).to(device),batch[:,1:input_n,dim_used]-batch[:,:(input_n-1),dim_used]),1)\n",
        "          sequences_gt=batch[:,10:35,:]\n",
        "\n",
        "\n",
        "          running_time = time.time()\n",
        "          sequences_predict1 = model(sequences_train1)\n",
        "\n",
        "          # COME BACK POSITIONAL REPRESENTATION OF THE FIRST PREDICTION:\n",
        "          # SUM EACH FRAME TO THE NEXT\n",
        "          # X_{N+1} = Y_{N+1} + Y_{N}\n",
        "          # ADD THE LAST FRAME OF THE TRAIN ONES\n",
        "          sequences_predict1[:,1:output_n,:]=sequences_predict1[:,1:output_n,:]+sequences_predict1[:,:(output_n-1),:]\n",
        "          sequences_predict1=(sequences_predict1+batch[:,(input_n-1):input_n,dim_used])\n",
        "\n",
        "          # GET SPEED REPRESENTATION OF THE PREDICTED STEPS:\n",
        "          # SET THE FIRST FRAME TO ZERO AND CALCULATE THE SPEED AS\n",
        "          # Y_{N+1} = X_{N+1} - X_{N}\n",
        "          sequences_train2=torch.cat((torch.zeros(*batch[:,:1,dim_used].size()).to(device),\n",
        "                                     sequences_predict1[:,6:15,:]-sequences_predict1[:,5:14,:]),1)\n",
        "\n",
        "          sequences_predict2=model(sequences_train2)\n",
        "\n",
        "          # COME BACK POSITIONAL REPRESENTATION OF THE SECOND PREDICTION:\n",
        "          # SUM EACH FRAME TO THE NEXT\n",
        "          # X_{N+1} = Y_{N+1} + Y_{N}\n",
        "          # ADD THE LAST FRAME OF THE TRAIN ONES\n",
        "          sequences_predict2[:,1:output_n,:]=sequences_predict2[:,1:output_n,:]+sequences_predict2[:,:(output_n-1),:]\n",
        "          sequences_predict2=(sequences_predict2+sequences_predict1[:,(output_n-1):output_n,:])\n",
        "\n",
        "          # STACK THE TWO PREDICTED SEQUENCES\n",
        "          sequences_predict = torch.cat((sequences_predict1,sequences_predict2[:,:10,:]),1)\n",
        "\n",
        "          totalll += time.time()-running_time\n",
        "          counter += 1\n",
        "\n",
        "          all_joints_seq[:,:,dim_used] = sequences_predict\n",
        "\n",
        "\n",
        "          all_joints_seq[:,:,index_to_ignore] = all_joints_seq[:,:,index_to_equal]\n",
        "\n",
        "          loss = mpjpe_error(all_joints_seq.view(-1,25,32,3),sequences_gt.view(-1,25,32,3))\n",
        "          running_loss += loss*batch_dim\n",
        "          accum_loss += loss*batch_dim\n",
        "\n",
        "      #print('loss at test subject for action : '+str(action)+ ' is: '+ str(running_loss/n))\n",
        "      print(str(action),': ', str(np.round((running_loss/n).item(),1)))\n",
        "      n_batches+=n\n",
        "    print('Average: ' + str(np.round((accum_loss/n_batches).item(),1)))\n",
        "    print('Prediction time: ', totalll/counter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ju5nNJsTcen"
      },
      "outputs": [],
      "source": [
        "path = './data/h3.6m/h3.6m/dataset'\n",
        "skip_rate = 1\n",
        "batch_size_test = 8\n",
        "actions_to_consider_test = 'all'\n",
        "ckpt_path = './checkpoints/LSTM_final_checkpoint_5.pt'\n",
        "\n",
        "test(ckpt_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "R2xJCGqoloFt",
        "NHIPyFkRl24A",
        "PPogCINwmIgb",
        "PY2CT5WZmNKI",
        "zUL3vfCfmaOO",
        "RStWekR7AVRB",
        "j7uoY-P7AVRC",
        "1nGPOuEoAVRC",
        "BUD8j4miTcek",
        "WiaFdbcWTcel",
        "4GKuQclGTcel"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
