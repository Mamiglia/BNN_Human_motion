{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import h36motion3d as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from Autoformer_main.models.Autoformer import Model as autoformer \n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.autograd\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils.loss_funcs import *\n",
    "from utils.data_utils import define_actions\n",
    "from utils.h36_3d_viz import *\n",
    "import time\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "### To a better printing\n",
    "from tabulate import tabulate\n",
    "\n",
    "from utils import h36motion3d as datasets\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from  import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Arguments to setup the datasets\n",
    "datas = 'h36m' # dataset name\n",
    "path = './data/h3.6m/h3.6m/dataset'\n",
    "input_n=10 # number of frames to train on (default=10)\n",
    "output_n=25 # number of frames to predict on\n",
    "input_dim=3 # dimensions of the input coordinates(default=3)\n",
    "skip_rate=1 # # skip rate of frames\n",
    "joints_to_consider=22\n",
    "\n",
    "\n",
    "#FLAGS FOR THE TRAINING\n",
    "mode='train' #choose either train or test mode\n",
    "\n",
    "batch_size_test=8\n",
    "model_path= './checkpoints/' # path to the model checkpoint file\n",
    "\n",
    "actions_to_consider_test='all' # actions to test on.\n",
    "model_name = datas+'_3d_'+str(output_n)+'frames_ckpt' #the model name to save/load\n",
    "\n",
    "#FLAGS FOR THE VISUALIZATION\n",
    "actions_to_consider_viz='all' # actions to visualize\n",
    "visualize_from='test'\n",
    "n_viz=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Train Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paolo\\Untitled Folder\\AML\\final_project\\autoformer\\utils\\h36motion3d.py:30: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  subs = np.array([[1, 6, 7, 8, 9], [11], [5]]) # , 6, 7, 8, 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Validation Dataset...\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "print('Loading Train Dataset...')\n",
    "dataset = datasets.Datasets(path,input_n,output_n,skip_rate, split=0)\n",
    "print('Loading Validation Dataset...')\n",
    "vald_dataset = datasets.Datasets(path,input_n,output_n,skip_rate, split=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training dataset length: 180077\n",
      ">>> Validation dataset length: 28110\n"
     ]
    }
   ],
   "source": [
    "batch_size=256\n",
    "\n",
    "print('>>> Training dataset length: {:d}'.format(dataset.__len__()))\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)#\n",
    "\n",
    "print('>>> Validation dataset length: {:d}'.format(vald_dataset.__len__()))\n",
    "vald_loader = DataLoader(vald_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configs:\n",
    "    def __init__(self, seq_len, pred_len, output_attention=False, moving_avg=3):\n",
    "        self.seq_len = seq_len\n",
    "        self.label_len = seq_len//2\n",
    "        self.pred_len = pred_len\n",
    "        self.output_attention = output_attention\n",
    "        self.moving_avg = moving_avg\n",
    "        self.enc_in=66\n",
    "        self.dec_in=66\n",
    "        self.d_model=66\n",
    "        self.embed='fixed'\n",
    "        self.freq='h'\n",
    "        self.dropout=0.1\n",
    "        self.e_layers=1\n",
    "        self.d_layers=1\n",
    "        self.c_out=66\n",
    "        self.factor=1\n",
    "        self.n_heads=1\n",
    "        self.d_ff=16\n",
    "        self.activation=\"relu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Configs(seq_len=10, pred_len=15)\n",
    "autof = autoformer(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments to setup the optimizer\n",
    "lr=1e-04 # learning rate\n",
    "use_scheduler=True # use MultiStepLR scheduler\n",
    "milestones=[10,30]   # the epochs after which the learning rate is adjusted by gamma\n",
    "gamma=0.1 #gamma correction to the learning rate, after reaching the milestone epochs\n",
    "weight_decay=1e-05 # weight decay (L2 penalty)\n",
    "optimizer=optim.Adam(autof.parameters(),lr=lr,weight_decay=weight_decay)\n",
    "device = \"cpu\"\n",
    "\n",
    "if use_scheduler:\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
    "\n",
    "clip_grad=None # select max norm to clip gradients\n",
    "# Argument for training\n",
    "n_epochs=41\n",
    "log_step=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader,vald_loader, path_to_save_model=None):\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "  val_loss_best = 1000\n",
    "\n",
    "  dim_used = np.array([6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 21, 22, 23, 24, 25,\n",
    "                    26, 27, 28, 29, 30, 31, 32, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45,\n",
    "                    46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 63, 64, 65, 66, 67, 68,\n",
    "                    75, 76, 77, 78, 79, 80, 81, 82, 83, 87, 88, 89, 90, 91, 92])\n",
    "\n",
    "  for epoch in range(n_epochs-1):\n",
    "      running_loss=0\n",
    "      n=0\n",
    "      autof.train()\n",
    "      for cnt,batch in enumerate(data_loader):\n",
    "          batch=batch.float().to(device)\n",
    "          batch_dim=batch.shape[0]\n",
    "          n+=batch_dim\n",
    "\n",
    "          sequences_train=torch.cat((torch.zeros(*batch[:,:1,dim_used].size()).to(device),batch[:,1:10,dim_used]-batch[:,:9,dim_used]),1)\n",
    "          sequences_gt=batch[:,10:25,dim_used]\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "          sequences_predict, kl = autof(sequences_train)\n",
    "          sequences_predict[:,1:15,:]=sequences_predict[:,1:15,:]+sequences_predict[:,:14,:]\n",
    "          sequences_predict=sequences_predict+batch[:,9:10,dim_used]\n",
    "\n",
    "          loss=mpjpe_error(sequences_predict,sequences_gt)\n",
    "\n",
    "          if cnt % log_step == 0:\n",
    "            print('[Epoch: %d, Iteration: %5d]  training mpjpe: %.3f, training kl: %.3f, training loss: %.3f' %(epoch + 1, cnt + 1, loss.item(), kl, loss.item()+kl))\n",
    "          loss += kl\n",
    "\n",
    "          loss.backward()\n",
    "          if clip_grad is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(autof.parameters(),clip_grad)\n",
    "\n",
    "          optimizer.step()\n",
    "          running_loss += loss*batch_dim\n",
    "\n",
    "      train_loss.append(running_loss.detach().cpu()/n)\n",
    "      autof.eval()\n",
    "      with torch.no_grad():\n",
    "          running_loss=0\n",
    "          n=0\n",
    "          for cnt,batch in enumerate(vald_loader):\n",
    "              batch=batch.float().to(device)\n",
    "              batch_dim=batch.shape[0]\n",
    "              n+=batch_dim\n",
    "\n",
    "              sequences_train=torch.cat((torch.zeros(*batch[:,:1,dim_used].size()).to(device),batch[:,1:10,dim_used]-batch[:,:9,dim_used]),1)\n",
    "              sequences_gt=batch[:,10:25,dim_used]\n",
    "\n",
    "              sequences_predict, kl = autof(sequences_train)\n",
    "              sequences_predict[:,1:15,:]=sequences_predict[:,1:15,:]+sequences_predict[:,:14,:]\n",
    "              sequences_predict=sequences_predict+batch[:,9:10,dim_used]\n",
    "\n",
    "\n",
    "              loss=mpjpe_error(sequences_predict,sequences_gt)\n",
    "\n",
    "              if cnt % log_step == 0:\n",
    "                print('[Epoch: %d, Iteration: %5d]  training mpjpe: %.3f, training kl: %.3f, training loss: %.3f' %(epoch + 1, cnt + 1, loss.item(), kl, loss.item()+kl))\n",
    "              loss += kl\n",
    "\n",
    "              running_loss+=loss*batch_dim\n",
    "          val_loss.append(running_loss.detach().cpu()/n)\n",
    "          if running_loss/n < val_loss_best:\n",
    "            val_loss_best = running_loss/n\n",
    "\n",
    "      if use_scheduler:\n",
    "        scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1, Iteration:     1]  training mpjpe: 79.593, training kl: 7.529, training loss: 87.121\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvald_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(data_loader, vald_loader, path_to_save_model)\u001b[0m\n\u001b[0;32m     31\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[Epoch: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, Iteration: \u001b[39m\u001b[38;5;132;01m%5d\u001b[39;00m\u001b[38;5;124m]  training mpjpe: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m, training kl: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m, training loss: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, cnt \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem(), kl, loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m+\u001b[39mkl))\n\u001b[0;32m     32\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m kl\n\u001b[1;32m---> 34\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clip_grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m   torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(autof\u001b[38;5;241m.\u001b[39mparameters(),clip_grad)\n",
      "File \u001b[1;32mc:\\Users\\paolo\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paolo\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(data_loader,vald_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
